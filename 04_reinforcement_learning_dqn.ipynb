{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.nervanasys.com/demystifying-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pong environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PongEnv(object):\n",
    "    def __init__(self, n_states=4):\n",
    "        self._env = gym.make(\"Pong-v0\")\n",
    "        # self._env.get_action_meanings()\n",
    "        # ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
    "        self._action_map = [2, 3]\n",
    "        self.n_states = n_states\n",
    "        self._states = deque(maxlen=self.n_states)\n",
    "        self.reset()\n",
    "    \n",
    "    def _preprocess_image(self, img):\n",
    "        # convert to grayscale\n",
    "        img = np.dot(img[...,:3], [0.299, 0.587, 0.114])\n",
    "        # resize\n",
    "        img = imresize(img, [60,60])\n",
    "        \n",
    "        # scale to 0..1\n",
    "        img = img / 128.0 - 1\n",
    "        \n",
    "        # img = np.expand_dims(img, axis=-1) # add single channel to make it TF friendly\n",
    "        return img.astype(np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self._env.reset()\n",
    "        s = self._preprocess_image(s)\n",
    "        \n",
    "        self._states.clear()\n",
    "        for _ in range(self.n_states - 1):\n",
    "            self._states.append(np.zeros_like(s))\n",
    "        self._states.append(s)\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    @property\n",
    "    def state(self):\n",
    "        s = np.array(self._states)\n",
    "        s = np.transpose(s, [1,2,0]) # CHW -> HWC\n",
    "        return s\n",
    "        \n",
    "    def step(self, action):\n",
    "        a = self._action_map[action]\n",
    "        s, r, done, info = self._env.step(a)\n",
    "        # debug\n",
    "        if r < 0:\n",
    "            done = True\n",
    "        s = self._preprocess_image(s)\n",
    "        self._states.append(s)\n",
    "        return self.state, r, done, info\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return gym.spaces.discrete.Discrete(len(self._action_map))\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        s = self.state\n",
    "        return gym.spaces.box.Box(s.min(),\n",
    "                                  s.max(),\n",
    "                                  s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-21 22:19:46,517] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = PongEnv()\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_small_fc_q_net(states, n_actions):\n",
    "    \"\"\"Build small fully connected Q-network\"\"\"\n",
    "    states_flat = slim.flatten(states)\n",
    "    layer1 = slim.fully_connected(states_flat,\n",
    "                                  64,\n",
    "                                  activation_fn=tf.nn.relu, scope='layer1')\n",
    "    q = slim.fully_connected(layer1,\n",
    "                             n_actions,\n",
    "                             activation_fn=None, scope='layer2')\n",
    "    return q, [layer1]\n",
    "\n",
    "def build_large_fc_q_net(states, n_actions):\n",
    "    \"\"\"Build 'large' fully connected Q-network\"\"\"\n",
    "    states_flat = slim.flatten(states)\n",
    "    layer1 = slim.fully_connected(states_flat,\n",
    "                                  200,\n",
    "                                  activation_fn=tf.nn.relu, scope='layer1')\n",
    "    q = slim.fully_connected(layer1,\n",
    "                             n_actions,\n",
    "                             activation_fn=None, scope='layer2')\n",
    "    return q, [layer1]\n",
    "\n",
    "def build_cnn_q_net(states, n_actions):\n",
    "    \"\"\"Build CNN Q-network\"\"\"\n",
    "    initializer = tf.truncated_normal_initializer(0, 0.1)\n",
    "    l1 = slim.conv2d(states, 32, [8, 8], stride=4, padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, weights_initializer=initializer, scope='layer1')\n",
    "    l2 = slim.conv2d(l1, 64, [4, 4], stride=2, padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, weights_initializer=initializer, scope='layer2')\n",
    "    l2_flat = slim.flatten(l2)\n",
    "    l3 = slim.fully_connected(l2_flat, 512, activation_fn=tf.nn.relu,\n",
    "                              weights_initializer=initializer, scope='layer3')\n",
    "    q = slim.fully_connected(l3, n_actions, activation_fn=None,\n",
    "                             weights_initializer=initializer, scope='q_layer')\n",
    "    return q, [l1, l2, l2_flat, l3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, n_actions, state_shape, discount=0.99, max_reward=1.0,\n",
    "                 lr_decay_factor=0.9, init_lr=1e-3, lr_decay_steps=1e3, lr_min=1e-7,\n",
    "                 min_epsilon=0.05, epsilon_decay_duration=1e4, start_training=1e3,\n",
    "                 train_freq=1,\n",
    "                 q_net_builder_fn=build_small_fc_q_net,\n",
    "                 summary_update_frequency=1000,\n",
    "                 mem_capacity=10000, batch_size=8, logs_dir='./dqn_logs'):\n",
    "        self.n_actions = n_actions\n",
    "        self.discount = discount\n",
    "        self.max_reward = max_reward\n",
    "        self.start_training = start_training\n",
    "        self.train_freq = train_freq\n",
    "        self.summary_update_frequency = summary_update_frequency\n",
    "        self.mem_capacity = mem_capacity\n",
    "        self.mem = []\n",
    "        self.batch_size = batch_size\n",
    "        self.graph = tf.Graph()\n",
    "        self.i = 0\n",
    "        with self.graph.as_default():\n",
    "            # add batch dimension\n",
    "            state_shape = [None] + list(state_shape)\n",
    "            \n",
    "            with tf.name_scope('inputs'):\n",
    "                self.state = tf.placeholder(tf.float32, shape=state_shape)\n",
    "                self.action = tf.placeholder(tf.int32, shape=[None])\n",
    "                # self.reward = tf.placeholder(tf.float32, shape=[None])\n",
    "                # self.state_ = tf.placeholder(tf.float32, shape=state_shape)\n",
    "                # self.terminal = tf.placeholder(tf.float32, shape=[None])\n",
    "            \n",
    "            with tf.variable_scope('q_net'):\n",
    "                self.q, self.q_net = q_net_builder_fn(self.state, n_actions)\n",
    "            \n",
    "#             with tf.variable_scope('q_net', reuse=True):\n",
    "#                 self.q_t = q_net_builder_fn(self.state_, n_actions)\n",
    "#                 # self.q_t = tf.stop_gradient(self.q_t) # we do not train q_target_net\n",
    "            \n",
    "            with tf.name_scope('loss'):\n",
    "                # debug\n",
    "                self.q_target = tf.placeholder(tf.float32, shape=[None])\n",
    "                # q_t_max = tf.reduce_max(self.q_t, reduction_indices=-1)\n",
    "                # q_target = (1. - self.terminal) * self.discount * q_t_max + self.reward\n",
    "                \n",
    "                mask = tf.one_hot(self.action, n_actions, dtype=tf.float32)\n",
    "                q_acted = tf.reduce_sum(self.q * mask, reduction_indices=-1)\n",
    "                # idx = tf.range(0, limit=n_actions*batch_size, delta=n_actions) + self.action\n",
    "                # q_flat = tf.reshape(self.q, shape=[-1])\n",
    "                # q_acted = tf.gather(q_flat, idx, validate_indices=True)\n",
    "                                \n",
    "                assert self.q_target.get_shape().is_compatible_with(q_acted.get_shape())\n",
    "                \n",
    "                diff = q_acted - self.q_target\n",
    "                tf.histogram_summary(\"diff\", diff)\n",
    "                # debug\n",
    "#                 clipped = tf.select(tf.abs(diff) < max_reward,\n",
    "#                                     0.5 * tf.square(diff),\n",
    "#                                     tf.abs(diff) - 0.5, name='clipped')\n",
    "                clipped = tf.square(diff)\n",
    "                self.error = tf.reduce_sum(clipped)\n",
    "                tf.scalar_summary(\"error\", self.error)\n",
    "            \n",
    "            with tf.name_scope('trainer'):\n",
    "                self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "                \n",
    "                global_step_1 = self.global_step + 1\n",
    "                self.global_step_inc = self.global_step.assign(global_step_1)\n",
    "                \n",
    "                exp_lr = tf.train.exponential_decay(init_lr,\n",
    "                                                    self.global_step,\n",
    "                                                    lr_decay_steps,\n",
    "                                                    lr_decay_factor,\n",
    "                                                    staircase=True)\n",
    "                \n",
    "                self.lr = tf.maximum(lr_min, exp_lr)\n",
    "                tf.scalar_summary(\"learning_rate\", self.lr)\n",
    "                # debug\n",
    "                # opt = tf.train.RMSPropOptimizer(self.lr, momentum=0.95, epsilon=0.01)\n",
    "                opt = tf.train.AdamOptimizer(self.lr)\n",
    "                self.train_op = opt.minimize(self.error)\n",
    "                \n",
    "                # piggy back on lr to update epsilon (exploration probability)\n",
    "                gs_float = tf.cast(self.global_step, tf.float32)\n",
    "                self.epsilon = 1.0 - gs_float / epsilon_decay_duration # start with 1.0 and decay\n",
    "                self.epsilon = tf.maximum(min_epsilon, self.epsilon)\n",
    "                tf.scalar_summary(\"epsilon\", self.epsilon)\n",
    "            \n",
    "            with tf.name_scope('summary'):\n",
    "                tf.histogram_summary('q_val', self.q)\n",
    "                # tf.histogram_summary('action', tf.argmax(self.q, axis=1))\n",
    "                tf.histogram_summary('q_target_val', self.q_target)\n",
    "                if len(self.state.get_shape()) == 4:\n",
    "                    tf.image_summary('states', self.state, max_images=4)\n",
    "                \n",
    "                self.total_reward = tf.placeholder(tf.float32, shape=())\n",
    "                self.game_steps = tf.placeholder(tf.float32, shape=())\n",
    "                ema = tf.train.ExponentialMovingAverage(0.5)\n",
    "                self.ema_op = ema.apply([self.total_reward, self.game_steps])\n",
    "                tf.scalar_summary(\"total_reward_avg\", ema.average(self.total_reward))\n",
    "                tf.scalar_summary(\"game_steps_avg\", ema.average(self.game_steps))\n",
    "                \n",
    "                with tf.name_scope('q_net_weights'):\n",
    "                    q_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_net')\n",
    "                    for v in q_net_vars:\n",
    "                        name = v.op.name.replace('q_net/', 'q_net_weights/')\n",
    "                        tf.histogram_summary(name, v)\n",
    "                with tf.name_scope('q_net_activations'):\n",
    "                    for o in self.q_net:\n",
    "                        name = o.op.name.replace('q_net/', 'q_net_activations/')\n",
    "                        tf.histogram_summary(name + '/activation', o)\n",
    "                        # tf.scalar_summary(o.op.name + '/sparsity', tf.nn.zero_fraction(o))\n",
    "\n",
    "                self.summary_op = tf.merge_all_summaries()\n",
    "                \n",
    "            init = tf.global_variables_initializer()\n",
    "            self.saver = tf.train.Saver(max_to_keep=20)\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        self.logs_dir = logs_dir\n",
    "        self.summary_writer = tf.train.SummaryWriter(self.logs_dir, graph=self.graph)\n",
    "        self.checkpoint_path = path.join(self.logs_dir, 'model.ckpt')\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(self.logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "            print('restoring at global_step = %d' % self.sess.run(self.global_step))\n",
    "                \n",
    "    def predict_action(self, state):\n",
    "        ep_val, self.i = self.sess.run([self.epsilon, self.global_step_inc])\n",
    "        if np.random.rand() < ep_val:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        state = np.expand_dims(state, 0)\n",
    "        q_val = self.sess.run(self.q, feed_dict={self.state: state})[0]\n",
    "        return np.argmax(q_val)\n",
    "    \n",
    "    def observe(self, state, action, reward, state_, terminal):\n",
    "        self.mem.append((state, action, reward, state_, terminal))\n",
    "        if len(self.mem) > self.mem_capacity:\n",
    "            self.mem.pop(0)\n",
    "    \n",
    "    def train(self):\n",
    "        if self.batch_size > len(self.mem): return\n",
    "        if self.i % self.train_freq != 0: return\n",
    "        \n",
    "        samples = random.sample(self.mem, self.batch_size)\n",
    "        s, a, r, s_, t = zip(*samples)\n",
    "        \n",
    "        s = np.array(s, dtype=np.float32)\n",
    "        s_ = np.array(s_, dtype=np.float32)\n",
    "        \n",
    "        # debug\n",
    "        sess = self.sess\n",
    "        r = np.array(r, dtype=np.float32)\n",
    "        t = np.array(t, dtype=int)\n",
    "        q_t = sess.run(self.q, {self.state: s_})\n",
    "        \n",
    "        q_t_max = np.max(q_t, axis=-1)\n",
    "        q_target = (1. - t) * self.discount * q_t_max + r\n",
    "                \n",
    "        feed_dict = {self.state: s,\n",
    "                     self.action: a,\n",
    "                     # self.reward: r,\n",
    "                     # self.state_: s_,\n",
    "                     # self.terminal: t\n",
    "                     self.q_target: q_target\n",
    "                    }\n",
    "        \n",
    "        \n",
    "        if True or self.i >= self.start_training:\n",
    "            _, err_val = sess.run([self.train_op, self.error], feed_dict)\n",
    "            assert not np.isnan(err_val), 'Model diverged with loss = NaN'\n",
    "        \n",
    "        if self.i % self.summary_update_frequency == 0:\n",
    "            summary_str = sess.run(self.summary_op, feed_dict)\n",
    "            self.summary_writer.add_summary(summary_str, self.i)\n",
    "            self.summary_writer.flush()\n",
    "        \n",
    "        if self.i % (5 * self.summary_update_frequency) == 0:\n",
    "            self.saver.save(sess,\n",
    "                            self.checkpoint_path,\n",
    "                            global_step=self.global_step)\n",
    "    \n",
    "    def summary_stats(self, total_reward, game_steps):\n",
    "        feed_dict = {self.total_reward: total_reward, \n",
    "                     self.game_steps: game_steps}\n",
    "        self.sess.run(self.ema_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf ./dqn_logs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent = Agent(env.action_space.n, env.observation_space.shape,\n",
    "#               start_training=0, max_reward=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(env.action_space.n, env.observation_space.shape,\n",
    "              max_reward=2.0, start_training=1e4,\n",
    "              lr_decay_steps=2e5, epsilon_decay_duration=2e5,\n",
    "              q_net_builder_fn=build_cnn_q_net, train_freq=4,\n",
    "              summary_update_frequency=5000, logs_dir='./dqn_logs_new_q_optim_delayed_start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47175/50000 [36:17:33<2:41:51,  3.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6f59618dd71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-33c5591a2a19>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Model diverged with loss = NaN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/regata/src/ml-experiments/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/regata/src/ml-experiments/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/regata/src/ml-experiments/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/regata/src/ml-experiments/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/regata/src/ml-experiments/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for game in tqdm(range(50000)):\n",
    "    s = env.reset()\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        a = agent.predict_action(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        agent.observe(s, a, r, s_, done)\n",
    "        s = s_\n",
    "        total_reward += r\n",
    "        agent.train()\n",
    "\n",
    "        steps += 1\n",
    "        if done:\n",
    "            agent.summary_stats(total_reward, steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s, a, r, s_, t = zip(*agent.mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61199999999999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0054999999999999997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(t).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = random.sample(agent.mem, agent.batch_size)\n",
    "\n",
    "s, a, r, s_, t = zip(*samples)\n",
    "        \n",
    "s = np.array(s, dtype=np.float32)\n",
    "s_ = np.array(s_, dtype=np.float32)\n",
    "        \n",
    "# debug\n",
    "sess = agent.sess\n",
    "r = np.array(r, dtype=np.float32)\n",
    "t = np.array(t, dtype=int)\n",
    "q_t = sess.run(agent.q, {agent.state: s_})\n",
    "\n",
    "q_t_max = np.max(q_t, axis=-1)\n",
    "q_target = (1. - t) * agent.discount * q_t_max + r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07994294, -0.102292  ],\n",
       "       [-0.12252456, -0.12732518],\n",
       "       [-0.69395757, -0.67708319],\n",
       "       [-0.19261444, -0.19532615],\n",
       "       [-0.17410833, -0.15542662],\n",
       "       [-0.08759212, -0.10275644],\n",
       "       [-0.66686499, -0.69425732],\n",
       "       [-0.12328768, -0.1318481 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = sess.run(agent.q, {agent.state: s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12744415, -0.13666874],\n",
       "       [-0.130696  , -0.10960853],\n",
       "       [-0.47380066, -0.34701461],\n",
       "       [-0.19242728, -0.20450848],\n",
       "       [-0.20565909, -0.18507099],\n",
       "       [-0.07686377, -0.09224242],\n",
       "       [-0.56720948, -0.57382506],\n",
       "       [-0.11671811, -0.12521726]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = q[range(agent.batch_size), a] - q_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04830064,  0.01169078,  0.32329776, -0.01382019, -0.03119864,\n",
       "        0.00985244,  0.09298686, -0.00316246])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11690904530230906"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diff**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11690905"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = sess.run(agent.error, {agent.state: s, agent.action: a, agent.q_target: q_target})\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4096)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = sess.run(agent.q_net[2], {agent.state: s})\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.25952131, 0.34683761, 8.1326871e-05)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = (im[0] - im[5])\n",
    "tmp.min(), tmp.max(), tmp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib so Gym's render() works nicely with Jupyter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12043e3c8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFhCAYAAACh/xvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkSEsSYkMYUlKZAhLUiJDWJIS/X/s2HH6YX8H7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1203659b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s, r, done, info = env.step(0)\n",
    "plt.imshow(s[...], cmap='gray', interpolation='none', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# observation, reward, done, info = env.step(2)\n",
    "# I = env.render(mode='rgb_array')\n",
    "# plt.imshow(I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
