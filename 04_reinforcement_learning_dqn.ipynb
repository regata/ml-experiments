{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.nervanasys.com/demystifying-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pong environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PongEnv(object):\n",
    "    def __init__(self):\n",
    "        self._env = gym.make(\"Pong-v0\")\n",
    "        # self._env.get_action_meanings()\n",
    "        # ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
    "        self._action_map = [2, 3] # only two actions 'RIGHT' and 'LEFT' matter\n",
    "        self.reset()\n",
    "        \n",
    "    def _preprocess_image(self, img):\n",
    "        \"\"\"Convert 210x160x3 uint8 frame into 40x40x1 float\n",
    "        Based on https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5#file-pg-pong-py-L30\n",
    "        \"\"\"\n",
    "        img = img[35:195] # crop\n",
    "        img = img[::4,::4,0] # downsample by factor of 4\n",
    "        img[img == 144] = 0 # erase background (background type 1)\n",
    "        img[img == 109] = 0 # erase background (background type 2)\n",
    "        img[img != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "        img = np.expand_dims(img, axis=2) # add single channel to make it TF friendly\n",
    "        return img.astype(np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self._env.reset()\n",
    "        s = self._preprocess_image(s)\n",
    "        self._prev_state = s\n",
    "        return np.zeros_like(s)\n",
    "        \n",
    "    def step(self, action):\n",
    "        a = self._action_map[action]\n",
    "        s, r, done, info = self._env.step(a)\n",
    "        s = self._preprocess_image(s)\n",
    "        x = s - self._prev_state\n",
    "        self._prev_state = s\n",
    "        return x, r, done, info\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return gym.spaces.discrete.Discrete(len(self._action_map))\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return gym.spaces.box.Box(self._prev_state.min(),\n",
    "                                  self._prev_state.max(),\n",
    "                                  self._prev_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-10 21:57:15,798] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# env = PongEnv()\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_small_fc_q_net(states, n_actions):\n",
    "    \"\"\"Build small fully connected Q-network\"\"\"\n",
    "    states_flat = slim.flatten(states)\n",
    "    layer1 = slim.fully_connected(states_flat,\n",
    "                                  64,\n",
    "                                  activation_fn=tf.nn.relu, scope='layer1')\n",
    "    q = slim.fully_connected(layer1,\n",
    "                             n_actions,\n",
    "                             activation_fn=None, scope='layer2')\n",
    "    return q\n",
    "\n",
    "def build_large_fc_q_net(states, n_actions):\n",
    "    \"\"\"Build 'large' fully connected Q-network\"\"\"\n",
    "    states_flat = slim.flatten(states)\n",
    "    layer1 = slim.fully_connected(states_flat,\n",
    "                                  200,\n",
    "                                  activation_fn=tf.nn.relu, scope='layer1')\n",
    "    layer2 = slim.fully_connected(layer1,\n",
    "                                  100,\n",
    "                                  activation_fn=tf.nn.relu, scope='layer2')\n",
    "    q = slim.fully_connected(layer2,\n",
    "                             n_actions,\n",
    "                             activation_fn=None, scope='layer3')\n",
    "    return q\n",
    "\n",
    "def build_cnn_q_net(states, n_actions):\n",
    "    \"\"\"Build CNN Q-network\"\"\"\n",
    "    conv1 = slim.conv2d(states, 32, [8, 8], stride=4, padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, scope='conv1')\n",
    "    conv2 = slim.conv2d(conv1, 16, [8, 8], stride=2, padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, scope='conv2')\n",
    "    conv2_flat = slim.flatten(conv2)\n",
    "    q = slim.fully_connected(conv2_flat, n_actions,\n",
    "                             activation_fn=None, scope='fc')\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, n_actions, state_shape, discount=0.99, max_reward=1.0,\n",
    "                 lr_decay_factor=0.9, init_lr=1e-3, lr_decay_steps=1e3, lr_min=1e-7,\n",
    "                 min_epsilon=0.05, epsilon_decay_duration=1e4,\n",
    "                 q_net_builder_fn=build_small_fc_q_net, update_q_target_frequency=1,\n",
    "                 summary_update_frequency=1000,\n",
    "                 mem_capacity=100000, batch_size=32):\n",
    "        self.n_actions = n_actions\n",
    "        self.discount = discount\n",
    "        self.max_reward = max_reward\n",
    "        self.update_q_target_frequency = update_q_target_frequency\n",
    "        self.summary_update_frequency = summary_update_frequency\n",
    "        self.mem_capacity = mem_capacity\n",
    "        self.mem = []\n",
    "        self.batch_size = batch_size\n",
    "        self.graph = tf.Graph()\n",
    "        self.i = 0\n",
    "        with self.graph.as_default():\n",
    "            # add batch dimension\n",
    "            state_shape = [None] + list(state_shape)\n",
    "            \n",
    "            with tf.name_scope('inputs'):\n",
    "                self.state = tf.placeholder(tf.float32, shape=state_shape)\n",
    "                self.action = tf.placeholder(tf.int32, shape=[None])\n",
    "                self.reward = tf.placeholder(tf.float32, shape=[None])\n",
    "                self.state_ = tf.placeholder(tf.float32, shape=state_shape)\n",
    "                self.terminal = tf.placeholder(tf.float32, shape=[None])\n",
    "            \n",
    "            with tf.variable_scope('q_net'):\n",
    "                self.q = q_net_builder_fn(self.state, n_actions)\n",
    "            \n",
    "            with tf.variable_scope('q_target_net'):\n",
    "                q_t = q_net_builder_fn(self.state_, n_actions)\n",
    "                self.q_t = tf.stop_gradient(q_t) # we do not train q_target_net\n",
    "            \n",
    "            with tf.name_scope('loss'):\n",
    "                q_t_max = tf.reduce_max(self.q_t, reduction_indices=-1)\n",
    "                q_target = (1. - self.terminal) * self.discount * q_t_max + self.reward\n",
    "                \n",
    "                mask = tf.one_hot(self.action, n_actions, dtype=tf.float32)\n",
    "                q_acted = tf.reduce_sum(self.q * mask, reduction_indices=-1)\n",
    "                # idx = tf.range(0, limit=n_actions*batch_size, delta=n_actions) + self.action\n",
    "                # q_flat = tf.reshape(self.q, shape=[-1])\n",
    "                # q_acted = tf.gather(q_flat, idx, validate_indices=True)\n",
    "                                \n",
    "                assert q_target.get_shape().is_compatible_with(q_acted.get_shape())\n",
    "                \n",
    "                diff = q_target - q_acted\n",
    "                tf.histogram_summary(\"diff\", diff)\n",
    "                clipped = tf.select(tf.abs(diff) < max_reward,\n",
    "                                    0.5 * tf.square(diff),\n",
    "                                    tf.abs(diff) - 0.5, name='clipped')\n",
    "                \n",
    "                # debug\n",
    "                self.error = tf.reduce_mean(clipped)\n",
    "                # self.error = tf.reduce_mean(tf.square(diff))\n",
    "                tf.scalar_summary(\"error\", self.error)\n",
    "            \n",
    "            with tf.name_scope('update_q_target_net'):\n",
    "                q_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_net')\n",
    "                q_target_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_target_net')\n",
    "                assign_ops = []\n",
    "                assert len(q_net_vars) == len(q_target_net_vars)\n",
    "                for v1, v2 in zip(q_net_vars, q_target_net_vars):\n",
    "                    v1_name = v1.name.split('/',1)[1] # name without scope\n",
    "                    v2_name = v2.name.split('/',1)[1]\n",
    "                    assert v1_name == v2_name\n",
    "                    assign_op = v2.assign(v1)\n",
    "                    assign_ops.append(assign_op)\n",
    "                self.update_q_target_op = tf.group(*assign_ops)\n",
    "            \n",
    "            with tf.name_scope('trainer'):\n",
    "                self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "                \n",
    "                global_step_1 = self.global_step + 1\n",
    "                self.global_step_inc = self.global_step.assign(global_step_1)\n",
    "                \n",
    "                exp_lr = tf.train.exponential_decay(init_lr,\n",
    "                                                    self.global_step,\n",
    "                                                    lr_decay_steps,\n",
    "                                                    lr_decay_factor,\n",
    "                                                    staircase=True)\n",
    "                \n",
    "                self.lr = tf.maximum(lr_min, exp_lr)\n",
    "                tf.scalar_summary(\"learning_rate\", self.lr)\n",
    "                opt = tf.train.AdamOptimizer(self.lr)\n",
    "                self.train_op = opt.minimize(self.error)\n",
    "                \n",
    "                # piggy back on lr to update epsilon (exploration probability)\n",
    "                gs_float = tf.cast(self.global_step, tf.float32)\n",
    "                self.epsilon = 1.0 - gs_float / epsilon_decay_duration # start with 1.0 and decay\n",
    "                self.epsilon = tf.maximum(min_epsilon, self.epsilon)\n",
    "                tf.scalar_summary(\"epsilon\", self.epsilon)\n",
    "            \n",
    "            with tf.name_scope('summary'):\n",
    "                tf.histogram_summary('q_val', self.q)\n",
    "                tf.histogram_summary('q_target_val', self.q_t)\n",
    "                # tf.image_summary('states', self.state, max_images=2)\n",
    "                \n",
    "                self.total_reward = tf.placeholder(tf.float32, shape=())\n",
    "                self.game_steps = tf.placeholder(tf.float32, shape=())\n",
    "                ema = tf.train.ExponentialMovingAverage(0.9)\n",
    "                self.ema_op = ema.apply([self.total_reward, self.game_steps])\n",
    "                tf.scalar_summary(\"total_reward_avg\", ema.average(self.total_reward))\n",
    "                tf.scalar_summary(\"game_steps_avg\", ema.average(self.game_steps))\n",
    "                \n",
    "                with tf.name_scope('q_net_summary'):\n",
    "                    q_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_net')\n",
    "                    for v in q_net_vars:\n",
    "                        tf.histogram_summary(v.op.name + '/activations', v)\n",
    "                        # tf.scalar_summary(v.op.name + '/sparsity', tf.nn.zero_fraction(v))\n",
    "\n",
    "                self.summary_op = tf.merge_all_summaries()\n",
    "                \n",
    "            init = tf.global_variables_initializer()\n",
    "            self.saver = tf.train.Saver(max_to_keep=20)\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(init)\n",
    "        self.sess.run(self.update_q_target_op)\n",
    "        \n",
    "        self.logs_dir = './dqn_logs'\n",
    "        self.summary_writer = tf.train.SummaryWriter(self.logs_dir, graph=self.graph)\n",
    "        self.checkpoint_path = path.join(self.logs_dir, 'model.ckpt')\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(self.logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "            print('restoring at global_step = %d' % self.sess.run(self.global_step))\n",
    "                \n",
    "    def predict_action(self, state):\n",
    "        ep_val, self.i = self.sess.run([self.epsilon, self.global_step_inc])\n",
    "        if np.random.rand() < ep_val:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        state = np.expand_dims(state, 0)\n",
    "        q_val = self.sess.run(self.q, feed_dict={self.state: state})[0]\n",
    "        return np.argmax(q_val)\n",
    "    \n",
    "    def observe(self, state, action, reward, state_, terminal):\n",
    "        self.mem.append((state, action, reward, state_, terminal))\n",
    "        if len(self.mem) > self.mem_capacity:\n",
    "            self.mem.pop(0)\n",
    "    \n",
    "    def train(self):\n",
    "        if self.batch_size > len(self.mem):\n",
    "            return\n",
    "        \n",
    "        samples = random.sample(self.mem, self.batch_size)\n",
    "        s, a, r, s_, t = zip(*samples)\n",
    "        \n",
    "        s = np.array(s, dtype=np.float32)\n",
    "        s_ = np.array(s_, dtype=np.float32)\n",
    "                \n",
    "        feed_dict = {self.state: s,\n",
    "                     self.action: a,\n",
    "                     self.reward: r,\n",
    "                     self.state_: s_,\n",
    "                     self.terminal: t\n",
    "                    }\n",
    "        sess = self.sess\n",
    "        \n",
    "        _, err_val = sess.run([self.train_op, self.error], feed_dict)\n",
    "        \n",
    "        assert not np.isnan(err_val), 'Model diverged with loss = NaN'\n",
    "        \n",
    "        if self.i % self.summary_update_frequency == 0:\n",
    "            summary_str = sess.run(self.summary_op, feed_dict)\n",
    "            self.summary_writer.add_summary(summary_str, self.i)\n",
    "            self.summary_writer.flush()\n",
    "        \n",
    "        if self.i % self.update_q_target_frequency == 0:\n",
    "            sess.run(self.update_q_target_op)\n",
    "        \n",
    "        if self.i % (5 * self.summary_update_frequency) == 0:\n",
    "            self.saver.save(sess,\n",
    "                            self.checkpoint_path,\n",
    "                            global_step=self.global_step)\n",
    "    \n",
    "    def summary_stats(self, total_reward, game_steps):\n",
    "        feed_dict = {self.total_reward: total_reward, \n",
    "                     self.game_steps: game_steps}\n",
    "        self.sess.run(self.ema_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf ./dqn_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(env.action_space.n, env.observation_space.shape, max_reward=100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:42<00:00,  3.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for game in tqdm(range(500)):\n",
    "    s = env.reset()\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        a = agent.predict_action(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        agent.observe(s, a, r, s_, done)\n",
    "        s = s_\n",
    "        total_reward += r\n",
    "        agent.train()\n",
    "\n",
    "        steps += 1\n",
    "        if done or total_reward > 200:\n",
    "        # if done:\n",
    "            agent.summary_stats(total_reward, steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # import matplotlib so Gym's render() works nicely with Jupyter\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# observation, reward, done, info = env.step(2)\n",
    "# I = env.render(mode='rgb_array')\n",
    "# plt.imshow(I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
