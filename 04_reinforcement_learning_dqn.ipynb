{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.nervanasys.com/demystifying-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pong environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PongEnv(object):\n",
    "    def __init__(self, n_states=4):\n",
    "        self._env = gym.make(\"Pong-v0\")\n",
    "        # self._env.get_action_meanings()\n",
    "        # ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
    "        self._action_map = [2, 3]\n",
    "        self.n_states = n_states\n",
    "        self._states = deque(maxlen=self.n_states)\n",
    "        self.reset()\n",
    "    \n",
    "    def _preprocess_image(self, img):\n",
    "        # convert to grayscale\n",
    "        img = np.dot(img[...,:3], [0.299, 0.587, 0.114])\n",
    "        # scale to 0..1\n",
    "        img = img / 128.0 - 1\n",
    "        # resize\n",
    "        img = imresize(img, [84,84])\n",
    "        \n",
    "        # img = np.expand_dims(img, axis=-1) # add single channel to make it TF friendly\n",
    "        return img.astype(np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self._env.reset()\n",
    "        s = self._preprocess_image(s)\n",
    "        \n",
    "        self._states.clear()\n",
    "        for _ in range(self.n_states - 1):\n",
    "            self._states.append(np.zeros_like(s))\n",
    "        self._states.append(s)\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    @property\n",
    "    def state(self):\n",
    "        s = np.array(self._states)\n",
    "        s = np.transpose(s, [1,2,0]) # CHW -> HWC\n",
    "        return s\n",
    "        \n",
    "    def step(self, action):\n",
    "        a = self._action_map[action]\n",
    "        s, r, done, info = self._env.step(a)\n",
    "        s = self._preprocess_image(s)\n",
    "        self._states.append(s)\n",
    "        return self.state, r, done, info\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return gym.spaces.discrete.Discrete(len(self._action_map))\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        s = self.state\n",
    "        return gym.spaces.box.Box(s.min(),\n",
    "                                  s.max(),\n",
    "                                  s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-17 16:49:24,193] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# env = PongEnv()\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_small_fc_q_net(states, n_actions):\n",
    "    \"\"\"Build small fully connected Q-network\"\"\"\n",
    "    states_flat = slim.flatten(states)\n",
    "    layer1 = slim.fully_connected(states_flat,\n",
    "                                  64,\n",
    "                                  activation_fn=tf.nn.relu, scope='layer1')\n",
    "    q = slim.fully_connected(layer1,\n",
    "                             n_actions,\n",
    "                             activation_fn=None, scope='layer2')\n",
    "    return q\n",
    "\n",
    "def build_large_fc_q_net(states, n_actions):\n",
    "    \"\"\"Build 'large' fully connected Q-network\"\"\"\n",
    "    states_flat = slim.flatten(states)\n",
    "    layer1 = slim.fully_connected(states_flat,\n",
    "                                  200,\n",
    "                                  activation_fn=tf.nn.relu, scope='layer1')\n",
    "    q = slim.fully_connected(layer1,\n",
    "                             n_actions,\n",
    "                             activation_fn=None, scope='layer2')\n",
    "    return q\n",
    "\n",
    "def build_cnn_q_net(states, n_actions):\n",
    "    \"\"\"Build CNN Q-network\"\"\"\n",
    "    initializer = tf.truncated_normal_initializer(0, 0.02)\n",
    "    conv1 = slim.conv2d(states, 32, [8, 8], stride=4, padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, scope='conv1')\n",
    "    conv2 = slim.conv2d(conv1, 64, [4, 4], stride=2, padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, scope='conv2')\n",
    "    conv3 = slim.conv2d(conv2, 64, [3, 3], stride=1, padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, scope='conv3')\n",
    "    conv_flat = slim.flatten(conv2)\n",
    "    l4 = slim.fully_connected(conv_flat, 512, activation_fn=tf.nn.relu, scope='layer4')\n",
    "    q = slim.fully_connected(l4, n_actions, activation_fn=None, scope='fc')\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, n_actions, state_shape, discount=0.99, max_reward=1.0,\n",
    "                 lr_decay_factor=0.9, init_lr=1e-3, lr_decay_steps=1e3, lr_min=1e-7,\n",
    "                 min_epsilon=0.05, epsilon_decay_duration=1e4,\n",
    "                 q_net_builder_fn=build_small_fc_q_net,\n",
    "                 summary_update_frequency=1000,\n",
    "                 mem_capacity=10000, batch_size=16):\n",
    "        self.n_actions = n_actions\n",
    "        self.discount = discount\n",
    "        self.max_reward = max_reward\n",
    "        self.summary_update_frequency = summary_update_frequency\n",
    "        self.mem_capacity = mem_capacity\n",
    "        self.mem = []\n",
    "        self.batch_size = batch_size\n",
    "        self.graph = tf.Graph()\n",
    "        self.i = 0\n",
    "        with self.graph.as_default():\n",
    "            # add batch dimension\n",
    "            state_shape = [None] + list(state_shape)\n",
    "            \n",
    "            with tf.name_scope('inputs'):\n",
    "                self.state = tf.placeholder(tf.float32, shape=state_shape)\n",
    "                self.action = tf.placeholder(tf.int32, shape=[None])\n",
    "                # self.reward = tf.placeholder(tf.float32, shape=[None])\n",
    "                # self.state_ = tf.placeholder(tf.float32, shape=state_shape)\n",
    "                # self.terminal = tf.placeholder(tf.float32, shape=[None])\n",
    "            \n",
    "            with tf.variable_scope('q_net'):\n",
    "                self.q = q_net_builder_fn(self.state, n_actions)\n",
    "            \n",
    "#             with tf.variable_scope('q_net', reuse=True):\n",
    "#                 self.q_t = q_net_builder_fn(self.state_, n_actions)\n",
    "#                 # self.q_t = tf.stop_gradient(self.q_t) # we do not train q_target_net\n",
    "            \n",
    "            with tf.name_scope('loss'):\n",
    "                # debug\n",
    "                self.q_target = tf.placeholder(tf.float32, shape=[None])\n",
    "                # q_t_max = tf.reduce_max(self.q_t, reduction_indices=-1)\n",
    "                # q_target = (1. - self.terminal) * self.discount * q_t_max + self.reward\n",
    "                \n",
    "                mask = tf.one_hot(self.action, n_actions, dtype=tf.float32)\n",
    "                q_acted = tf.reduce_sum(self.q * mask, reduction_indices=-1)\n",
    "                # idx = tf.range(0, limit=n_actions*batch_size, delta=n_actions) + self.action\n",
    "                # q_flat = tf.reshape(self.q, shape=[-1])\n",
    "                # q_acted = tf.gather(q_flat, idx, validate_indices=True)\n",
    "                                \n",
    "                assert self.q_target.get_shape().is_compatible_with(q_acted.get_shape())\n",
    "                \n",
    "                diff = self.q_target - q_acted\n",
    "                tf.histogram_summary(\"diff\", diff)\n",
    "                clipped = tf.select(tf.abs(diff) < max_reward,\n",
    "                                    0.5 * tf.square(diff),\n",
    "                                    tf.abs(diff) - 0.5, name='clipped')\n",
    "                \n",
    "                self.error = tf.reduce_mean(clipped)\n",
    "                tf.scalar_summary(\"error\", self.error)\n",
    "            \n",
    "            with tf.name_scope('trainer'):\n",
    "                self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "                \n",
    "                global_step_1 = self.global_step + 1\n",
    "                self.global_step_inc = self.global_step.assign(global_step_1)\n",
    "                \n",
    "                exp_lr = tf.train.exponential_decay(init_lr,\n",
    "                                                    self.global_step,\n",
    "                                                    lr_decay_steps,\n",
    "                                                    lr_decay_factor,\n",
    "                                                    staircase=True)\n",
    "                \n",
    "                self.lr = tf.maximum(lr_min, exp_lr)\n",
    "                tf.scalar_summary(\"learning_rate\", self.lr)\n",
    "                opt = tf.train.AdamOptimizer(self.lr)\n",
    "                self.train_op = opt.minimize(self.error)\n",
    "                \n",
    "                # piggy back on lr to update epsilon (exploration probability)\n",
    "                gs_float = tf.cast(self.global_step, tf.float32)\n",
    "                self.epsilon = 1.0 - gs_float / epsilon_decay_duration # start with 1.0 and decay\n",
    "                self.epsilon = tf.maximum(min_epsilon, self.epsilon)\n",
    "                tf.scalar_summary(\"epsilon\", self.epsilon)\n",
    "            \n",
    "            with tf.name_scope('summary'):\n",
    "                tf.histogram_summary('q_val', self.q)\n",
    "                # tf.histogram_summary('action', tf.argmax(self.q, axis=1))\n",
    "                tf.histogram_summary('q_target_val', self.q_target)\n",
    "                if len(self.state.get_shape()) == 4:\n",
    "                    tf.image_summary('states', self.state, max_images=4)\n",
    "                \n",
    "                self.total_reward = tf.placeholder(tf.float32, shape=())\n",
    "                self.game_steps = tf.placeholder(tf.float32, shape=())\n",
    "                ema = tf.train.ExponentialMovingAverage(0.9)\n",
    "                self.ema_op = ema.apply([self.total_reward, self.game_steps])\n",
    "                tf.scalar_summary(\"total_reward_avg\", ema.average(self.total_reward))\n",
    "                tf.scalar_summary(\"game_steps_avg\", ema.average(self.game_steps))\n",
    "                \n",
    "                with tf.name_scope('q_net_summary'):\n",
    "                    q_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_net')\n",
    "                    for v in q_net_vars:\n",
    "                        tf.histogram_summary(v.op.name + '/activations', v)\n",
    "                        # tf.scalar_summary(v.op.name + '/sparsity', tf.nn.zero_fraction(v))\n",
    "\n",
    "                self.summary_op = tf.merge_all_summaries()\n",
    "                \n",
    "            init = tf.global_variables_initializer()\n",
    "            self.saver = tf.train.Saver(max_to_keep=20)\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        self.logs_dir = './dqn_logs'\n",
    "        self.summary_writer = tf.train.SummaryWriter(self.logs_dir, graph=self.graph)\n",
    "        self.checkpoint_path = path.join(self.logs_dir, 'model.ckpt')\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(self.logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "            print('restoring at global_step = %d' % self.sess.run(self.global_step))\n",
    "                \n",
    "    def predict_action(self, state):\n",
    "        ep_val, self.i = self.sess.run([self.epsilon, self.global_step_inc])\n",
    "        if np.random.rand() < ep_val:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        state = np.expand_dims(state, 0)\n",
    "        q_val = self.sess.run(self.q, feed_dict={self.state: state})[0]\n",
    "        return np.argmax(q_val)\n",
    "    \n",
    "    def observe(self, state, action, reward, state_, terminal):\n",
    "        self.mem.append((state, action, reward, state_, terminal))\n",
    "        if len(self.mem) > self.mem_capacity:\n",
    "            self.mem.pop(0)\n",
    "    \n",
    "    def train(self):\n",
    "        if self.batch_size > len(self.mem):\n",
    "            return\n",
    "        \n",
    "        samples = random.sample(self.mem, self.batch_size)\n",
    "        s, a, r, s_, t = zip(*samples)\n",
    "        \n",
    "        s = np.array(s, dtype=np.float32)\n",
    "        s_ = np.array(s_, dtype=np.float32)\n",
    "        \n",
    "        # debug\n",
    "        sess = self.sess\n",
    "        r = np.array(r, dtype=float)\n",
    "        t = np.array(t, dtype=int)\n",
    "        q_t = sess.run(self.q, {self.state: s_})\n",
    "        \n",
    "        q_t_max = np.max(q_t, axis=-1)\n",
    "        q_target = (1. - t) * self.discount * q_t_max + r\n",
    "                \n",
    "        feed_dict = {self.state: s,\n",
    "                     self.action: a,\n",
    "                     # self.reward: r,\n",
    "                     # self.state_: s_,\n",
    "                     # self.terminal: t\n",
    "                     self.q_target: q_target\n",
    "                    }\n",
    "        \n",
    "        \n",
    "        _, err_val = sess.run([self.train_op, self.error], feed_dict)\n",
    "        \n",
    "        assert not np.isnan(err_val), 'Model diverged with loss = NaN'\n",
    "        \n",
    "        if self.i % self.summary_update_frequency == 0:\n",
    "            summary_str = sess.run(self.summary_op, feed_dict)\n",
    "            self.summary_writer.add_summary(summary_str, self.i)\n",
    "            self.summary_writer.flush()\n",
    "        \n",
    "        if self.i % (5 * self.summary_update_frequency) == 0:\n",
    "            self.saver.save(sess,\n",
    "                            self.checkpoint_path,\n",
    "                            global_step=self.global_step)\n",
    "    \n",
    "    def summary_stats(self, total_reward, game_steps):\n",
    "        feed_dict = {self.total_reward: total_reward, \n",
    "                     self.game_steps: game_steps}\n",
    "        self.sess.run(self.ema_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf ./dqn_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(env.action_space.n, env.observation_space.shape,\n",
    "              max_reward=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent = Agent(env.action_space.n, env.observation_space.shape,\n",
    "#               max_reward=2.0, start_training=1e4,\n",
    "#               lr_decay_steps=2e5, epsilon_decay_duration=2e5,\n",
    "#               q_net_builder_fn=build_cnn_q_net,\n",
    "#               summary_update_frequency=5000, logs_dir='./dqn_logs_new_q_optim_delayed_start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:36<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for game in tqdm(range(300)):\n",
    "    s = env.reset()\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        a = agent.predict_action(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        agent.observe(s, a, r, s_, done)\n",
    "        s = s_\n",
    "        total_reward += r\n",
    "        agent.train()\n",
    "\n",
    "        steps += 1\n",
    "        if done:\n",
    "            agent.summary_stats(total_reward, steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # import matplotlib so Gym's render() works nicely with Jupyter\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x, r, done, info = env.step(1)\n",
    "# plt.imshow(x.squeeze(), cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# observation, reward, done, info = env.step(2)\n",
    "# I = env.render(mode='rgb_array')\n",
    "# plt.imshow(I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
